如果$\frac{\partial C}{\partial z_j^l}​$绝对值很大，则可以通过选择和它反向（符号相反）的$\Delta z_j^l​$轻易使代价函数减小，相反的，如果$\frac{\partial C}{\partial z_j^l}​$很接近于零，就说明这个神经元很接近最优情况了。综上，我们可以认$\frac{\partial C}{\partial z_j^l}​$是一个对于神经元误差(error)的测量尺度。最后，我们定义误差第$l​$层第$j​$个神经元的误差$\delta_j^l ​$为$\frac{\partial C}{\partial z_j^l}​$。

$\delta^l$表示第$l$层的误差向量。反向传播算法会告诉我们怎么计算每层的$\delta^l$，并关联到我们真正感兴趣的问题-------$\frac{\partial C}{\partial w_{jk}^l}$和$\frac{\partial C}{\partial b_j^l}$。



反向传播算法基于四个基本的方程

**1.输出层误差$\delta^L$的方程（BP1）:**

==$\delta_j^L=\frac{\partial C}{\partial a_j^L}\delta'(z_j^L)​$==

> $\delta_j^L=\frac{\partial C}{\partial a_j^L}\delta'(z_j^L)=\frac{\partial C}{\partial a_j^L}\frac{\partial a_j^L}{\partial z_j^L}=\frac{\partial C}{\partial z_j^l}​$



注意，上式的每个部分都是容易计算的。根据代价函数$C=\frac{1}{2}\sum_j(y_j-a_j^L)^2​$，$\frac{\partial C}{\partial a_j^L}=(a_j^L-y_j)​$。$\delta'(z_j^L)​$亦。

BP1的矩阵形式为$\delta^L= \nabla_aC\odot\delta'(z^L)==(a^L-y)\odot\delta'(z^L)$，其中，$\nabla_aC$是C基于输出激励的梯度。



**2.误差$\delta^l$依赖下一层误差$delta^{l+1}$的方程（BP2）**

==$\delta^l=((w^{l+1})^T\delta^{l+1})\odot\delta'(z^l)$==

这个式子看起来复杂，但每个部分都有很好的替换。假设我们知道了$l+1$层的误差$\delta^{l+1}$，我们可以直观的认为这是把误差在网络上前移，给我们第$l$层输出误差的度量尺度。再来看看Hadamard成$\odot\delta'(z^l)$，这是把误差通过第$l$层的激励函数前移，给我们第$l$层在加权输入$z^l$上的误差$\delta^l$。

> 根据上文，误差是在$z$上的，单纯的误差前移是在$a$上的，为了把误差变成$z$上的，故乘$\delta'(z^l)$。



结合BP1和BP2，我们可以计算网络上每一层的误差$\delta^l$



**3.代价函数对于网络上每一个偏置的变化率的方程（BP3）**

==$\frac{\partial C}{\partial b_j^l}=\delta_j^l$==

就是说，误差$\delta_j^l$正好等于变化率$\frac{\partial C}{\partial b_j^l}$

**4.代价函数对于网络上每一个权重的变化率的方程（BP4）**

==$\frac{\partial C}{\partial w_{jk}^l}=a_k^{l-1}\delta_j^l$==

这个式子告诉我们如何用$\delta^l$和$a^{l-1}$来计算偏导$\frac{\partial C}{\partial w_{jk}^l}$ 。这个式子可以写成有更少标记的方式$\frac{\partial C}{\partial w}=a_{in}\delta_{out}$。

从这个方程我们能得到一个很棒的结论，当激励$a_{in}$很接近零时，梯度项$\frac{\partial C}{\partial w}$也倾向于变得更接近零。这种情况下，我们说权重学习的很慢，即梯度下降时它没有改变太多。换句话说，BP4的一个结论是，低激发的神经元做起点的权重学的慢。





四个方程的证明，关键就是对误差$\delta_j^l$的定义------改变$z_j^l$会对代价函数产生多大的改变，即$\frac{\partial C}{\partial z_j^l}$ ，然后利用链式法则就能轻易推导出四个方程。

 



### 反向传播算法

反向传播方程给我们提供了计算代价函数梯度的方法。下面我们把它写成明确的算法形式：

1. 输入$x$：设置输入层的激励$a^1$
2. 前调：对每一层，计算$z^l$和$a^l$。
3. 输出误差$\delta^L$：计算向量$\delta^L$，BP1
4. 反向传播误差：BP2
5. 输出：代价函数的梯度，BP3，BP4


underfitting 	  欠拟合------------对训练数据的拟合不够好

high bias		高偏差-------------同上

overfitting	      过拟合-------------对训练数据拟合非常好，但泛化能力极差

high variance	 高方差------------同上



## 过拟合

有太多的特征，和较少的训练数据，使学到的假设函数对训练数据拟合的太好了（$J(\theta)\dot= 0$），但对新的样本泛化（generalize）非常差

解决办法：

- 减少特征的数量
  - 人工挑选
  - 模型选择算法（稍后）
- 正则化（regularization）
  - -保留所有的特征，但减少数量级或参数数值的大小
  - 当我们保留很多特征时仍能工作的很好，且每个特征都能对y值做出影响。

### 正则化

![](.\pics\正则化.png)

#### 正则化的线性回归：

$J(\theta)=\frac{1}{2m}[\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum^{{\color{red}n}}_{j=1}\theta_j^2]$

> 如果$\lambda$ 过大，则除了$\theta_0$以外都趋于0，则将拟合成一条直线，欠拟合。
>
> $\lambda$的选择------------以后才会讲

则梯度下降法应做：

repeat{

​	$\theta_0:=\theta_0-\alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^i)​$    (因为没有对$\theta_0​$ 进行惩罚)

​	$\theta_j:=\theta_j-\alpha \frac{1}{m}[\sum_{i=1}^{m}((h_\theta(x^{(i)})-y^i)x_j^{(i)})+\lambda\theta_j]​$ 

}

则正规方程法应做：

![](.\pics\正则化-1.png)



#### 正则化的逻辑回归：

$J(\theta)=-\frac{1}{m}[\sum_{i=1}^m ylog(h_\theta(x^{(i)})+(1-y)log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2$

则梯度下降法应做：**同线性回归形式相同**

repeat{

​	$\theta_0:=\theta_0-\alpha \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^i)$    (因为没有对$\theta_0$ 进行惩罚)

​	$\theta_j:=\theta_j-\alpha \frac{1}{m}[\sum_{i=1}^{m}((h_\theta(x^{(i)})-y^i)x_j^{(i)})+\lambda\theta_j]$ 

}
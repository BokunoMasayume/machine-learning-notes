# 逻辑回归

> 使用线性回归来做分类不是一个好主意，举个栗子：
>
> ![](.\pics\线性回归分类.png)
>
> 原本拟合的挺好的直线在加了最右面的样本后立马变差了，你能得到正确的模型，只能说明你运气好
>
> 而且拟合的$h_\theta(x)$可能远远大于1，或远远 小于0，尽管在输入样本时y只标了0或1。
>
> 事实上，线性回归是想办法将点连在一起，逻辑回归是想办法把点分割开来。
>
> 二元逻辑回归的$h_\theta(x)$对于y值只可能是0或1的样本，将介于0~1之间，看起来比线性回归科学多了。
>
> 0------negative class---------'-'
>
> 1------positive class----------'+'

## 假设函数（Hypothesis）表达式

#### 逻辑函数/S型函数（Logistic function/sigmoid function）

$$
{\color{red}g(z)=\frac{1}{1+e^{-z}}}
$$

![](.\pics\s型函数.png)

逻辑回归数学建模：==$h_\theta(\vec{x})=g(\theta^T\vec{x})=\frac{1}{1+e^{-\theta^TX}}$==

**解释假设模式的输出**：$h_\theta(x)​$是预测的在输入是x的情况下`y=1`的概率

> 举个栗子：
>
> 假如$x=\begin{vmatrix}x_0\\x_1\end{vmatrix}=\begin{vmatrix}1\\tumorSize\end{vmatrix}$
>
> 且$h_\theta(x)=0.7​$
>
> 意思是有70%的概率这个肿瘤是恶性的

**即$h_\theta(x)=P(y=1|x;\theta)$：参数设为theta,对给定的x，y=1的概率**

易知，$P(y=0|x;\theta)=1-P(y=1|x;\theta)$

## 决策边界（decision boundary）

$h_\theta(x)=g(\theta^TX)=\frac{1}{1+e^{-\theta^TX}}$

对逻辑回归，当$h_\theta(x)\ge0.5$时，$y=1$

​			   $h_\theta(x)<0.5$时，$y=0$

对S型函数$g(z)​$，$z\ge0​$时，$g(z)\ge 0.5​$

​			     $z<0$时，$g(z)<0.5$

so.....the key is:

​	$\theta^TX\ge0$时，$y=1$

​	$\theta^TX<0$时，$y=0$ 

决策边界：$\theta^TX=0$这条线



----

非线性决策边境

引入特征的高阶做为新特征

for example:

![](.\pics\非线性决策边境.png)

**决策边境不是数据集的属性，而是假设本身及其参数的属性**



## 代价函数

convex------凸

$cost(h_\theta(x^{(i)}),y^{(i)})=\begin{cases}-log(h_\theta(x^{(i)})&\mbox{if }y^{(i)}=1\\-log(1-h_\theta(x^{(i)}))&\mbox{if }y^{(i)}=0\end{cases}​$

$J(\theta)=\frac{1}{m}\sum_{i=1}^{m}cost(h_\theta(x^{(i)}),y^{(i)})$

==cost-h图==

![](.\pics\逻辑回归-1.png)

![](.\pics\逻辑回归-0.png)

![](.\pics\逻辑回归.png)

**简化代价函数和梯度下降**

因为y的取值只能是0或1，所以代价函数可以简化为：

$cost(h_\theta(x^{(i)}),y^{(i)})=-ylog(h_\theta(x^{(i)})-(1-y)log(1-h_\theta(x^{(i)}))$

所以有：$​

$J(\theta)=-\frac{1}{m}[\sum_{i=1}^m y^{(i)}log(h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]$ 

为了拟合 $\theta​$：需要$min_\theta J(\theta)​$

则梯度下降法应做：

repeat{

​	$\theta_j := \theta_j-\alpha\frac{\partial}{\partial{\theta_j}}J(\theta)$ 

}

上式等效于

repeat{

​	$\theta_j:=\theta_j-\alpha \frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)})-y^i)x_j^{(i)})$ 

}

==迭代公式的矩阵表示方法为：==

> $\theta := \theta -\frac{\alpha}{m}X^T(g(X\theta)-y)$
>
> X：$(\vec{x^{(1)}},\vec{x^{(2)}},\vec{x^{(3)}},\vec{x^{(4)}}.... )^T$

> 可以看到其迭代公式形式与线性回归的相同，但要注意它们的假设函数$h_\theta (x)$不同。





## 高级优化算法

- 共轭梯度法
- 。。。
- 。。。



### 多分类分类器

**一对多方法：**

对k-类分类，分别搞一个以此类为正类的逻辑分类器

对输入x，哪个分类器输出（概率）高，就认为x是哪一类的。


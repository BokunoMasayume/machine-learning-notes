鸡尾酒会问题算法----octave程序

[W,s,v]=svd((repmat(sum(x.\*x,1),size(x,1),1).\*x)\*x');



### 梯度下降法：

$\theta_i\ :=\theta_i-\alpha\frac{\partial}{\partial \theta_i}J(\vec{\theta})$



$\alpha$：步长，学习速率



### 奇异矩阵（退化矩阵）

没有逆矩阵的矩阵--------据说它们非常近似于0



# 多元线性回归

标志意义：

- $n$ = 每个样本的特征数，即特征向量维数
- $m$ = 样本个数
- $x^{(i)}$ = 第$i$个样本的特征向量
- $x^{(i)}_j$ = 第$i$个样本的第$j$个特征



数学模型：

$h_{\theta}(\vec{x}) = \theta_0+\theta_1 x_1+\theta_2 x_2+...+\theta_nx_n$

我们定义$x_0=1$，因此

$\vec{x}=\begin{vmatrix}x_0\\x_1\\x_2\\...\\x_n\end{vmatrix}\in \R^{n+1}$   $\vec{\theta}=\begin{vmatrix}\theta_0\\\theta_1\\\theta_2\\...\\\theta_n\end{vmatrix}\in\R^{n+1}$

那么原来的公式可以表示为 $h_\theta(x)=\theta^Tx$ 

代价函数为：$J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2​$

则梯度下降法应做：

repeat{

​	$\theta_j := \theta_j-\alpha\frac{\partial}{\partial{\theta_j}}J(\theta)$ 

}

上式等效于

repeat{

​	$\theta_j:=\theta_j-\alpha \frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)})-y^i)x_j^{(i)})​$ 

}

一些小技巧：

- 特征缩放，将特征全都变为较小且比例近与1，较小是为了使每步走的大一些，比例近乎于一，则$J(\theta)$的轮廓图的等高线更圆一些，我们知道越圆的椭圆切线越指向圆心，少走弯路。

  - 通常将特征值缩放到$(-1,+1)$之间，（并不是硬性规定
  - 当然如果你的特征值太小了，也要适度放大

- 均值归一化（mean normalization）

  - 用$x_i-u_i$代替$x_i$，使数据集中该特征的平均值为0
  - 当然绝对不能用在$x_0$上

  所以，你可以先这么处理你的特征：

  - $u_i$是你第$i$个特征的平均值
  - $s_i$是你第$i$个特征的范围大小
  - 把你的特征值变为：$x_i:= \frac{x_i-u_i}{s_i}$

- 如何选择学习率$\alpha$ 

  - 先跟着直觉选一个$\alpha$，在每一步迭代都输出计算出的$J(\theta)$，如果$J(\theta)$一直变小，没有上下横跳或一路变大就说明这个学习率可以，你也可以根据情况适度调大它；
  - 如果$J(\theta)$随着迭代次数的变大忽大忽小或一路变大不复返，说明学习率选大了，要调小
  - 还有确定何时停止迭代，两次迭代计算的$J(\theta)$的差值小于$\delta$，就看做已到局部最优点，停止迭代

  !["J-iterator"](D:\md笔记\机器学习\吴恩达\pics\learning-rate.png)

  

如图就是$J(\theta)-\#iterator$的关系图，y轴到底会在哪里，以及曲线的缩放取决于你选择的初始$\theta$和学习率$\alpha$



## 特征选择

选择适当的特征会大大增加准确性



## 多项式回归

> 符号说明：
>
> $x^{(i)}$：数据集中第i个样本的特征
>
> $x^j​$：$x​$的$j​$次幂

#### 多项式回归：

单元多项式回归：

数学模型：$h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+...+\theta_nx^n$

 代价函数：$J(\theta)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$       

梯度下降法应做：

repeat{

​	$\theta_j:=\theta_j-\frac{\alpha}{m}\sum_{i=1}^{m}((h_\theta(x)^{(i)}-y^{(i)})x^{(i)^j})$

}

和多元线性回归相似，$x^j$相当于$x_j$

> 需要注意的是，多项式回归中，特征值的归一化变得很重要，因为幂运算会大大缩放原值。
>
> 比如 :
>
> ​	$x$的范围是 0~100
>
> ​	$x^2$的范围就会变成0~10,000
>
> 不做归一化处理将会使梯度下降过程变得很坎坷

所以既然单元多项式回归等价于特征向量是$(1,x,x^2,x^3...)^T$，则多元的多项式归回同理，特征向量是$(1,x_1,x_1^2...,x_2,x_2^2...,x_i^ax_j^b)^T$





# 正规方程（Normal Equation）

> 梯度下降法------------------数次迭代取得优化解，归一化与否会影响迭代效率
>
> 正规方程---------------------一步直接取得优化解，不需要归一化

一个函数的极值常常在导数等于零的位置，对于$n+1$维向量$\theta$，它在$J(\theta)$各个偏导都为零的位置，即：

​	$\frac{\partial}{\partial{\theta_i}}J(\theta)=0\  \ \ \ \forall i\in\ (0,n)​$

举例：

![正规方程](D:\md笔记\机器学习\吴恩达\pics\正规方程.png)

数据集集成一个$m\times (n+1)$矩阵，每一行表示一个样本，每一列表示各样本相应特征取值。

对$\theta​$的一项 :$\frac{\partial}{\partial{\theta_i}}J(\theta)=\frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)})-y^i)x_j^{(i)})=0​$

即$\sum_{i=1}^{m}((h_\theta(x^{(i)})-y^i)x_j^{(i)})=0$，即$x_i^T(X\theta-y)=0$（$x_i$为X的第i列）

则对$\theta$来说有$X^T(X\theta-y)=0$，即==$\theta=(X^TX)^{-1}X^Ty$==



| 梯度下降法                                  | 正规方程法                                                   |
| ------------------------------------------- | ------------------------------------------------------------ |
| 需要设置学习率$\alpha$                      | 无需设置$\alpha$                                             |
| 需要多次迭代                                | 无需迭代                                                     |
| 当特征数量（特征向量维度n）很大时仍工作良好 | 需要计算$(X^TX)$，时间复杂度为$O(n^3)$，特征数量越多，会指数级变慢 |

> 什么样的n算”很大“呢---------------------------或许至少要上万

#### 正规方程法的不可逆（Normal Equation Noninvertibility）

当满足一下某个条件时，$X^TX$将不可逆：

- 特征间有线性关系。比如$x_1$是英尺做单位的长度，$x_2$是米做单位的长度
  - 解决办法：删除和其他特征线性相关的特征
- 特征数量不大于样本数量
  - 解决办法
    - 删掉一个或多个特征
    - 使用正则化($\lambda>0$ ，给$X^TX$加了矩阵$\begin{bmatrix}0&0&0...\\0&1&0..\\0&0&1...\\...\end{bmatrix}$)，就不奇异了



> so....虽然这章的题目叫`线性回归`，但实际还介绍了多项式回归，和多元的情况，因为幂关系不构成线性关系，所以尽可以将$x_i,x_i^n,x_i^ax_j^b....$它们都列为特征--------------只要你样本够多的话。
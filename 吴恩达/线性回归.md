鸡尾酒会问题算法----octave程序

[W,s,v]=svd((repmat(sum(x.\*x,1),size(x,1),1).\*x)\*x');



### 梯度下降法：

$\theta_i\ :=\theta_i-\alpha\frac{\partial}{\partial \theta_i}J(\vec{\theta})$



$\alpha$：步长，学习速率



### 奇异矩阵（退化矩阵）

没有逆矩阵的矩阵--------据说它们非常近似于0



# 多元线性回归

标志意义：

- $n$ = 每个样本的特征数，即特征向量维数
- $m$ = 样本个数
- $x^{(i)}$ = 第$i$个样本的特征向量
- $x^{(i)}_j$ = 第$i$个样本的第$j$个特征



数学模型：

$h_{\theta}(\vec{x}) = \theta_0+\theta_1 x_1+\theta_2 x_2+...+\theta_nx_n$

我们定义$x_0=1$，因此

$\vec{x}=\begin{vmatrix}x_0\\x_1\\x_2\\...\\x_n\end{vmatrix}\in \R^{n+1}$   $\vec{\theta}=\begin{vmatrix}\theta_0\\\theta_1\\\theta_2\\...\\\theta_n\end{vmatrix}\in\R^{n+1}$

那么原来的公式可以表示为 $h_\theta(x)=\theta^Tx$ 

代价函数为：$J(\theta) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$

则梯度下降法应做：

repeat{

​	$\theta_j := \theta_j-\alpha\frac{\partial}{\partial{\theta_j}}J(\theta)$ 

}

上式等效于

repeat{

​	$\theta_j:=\theta_j-\alpha \frac{1}{m}\sum_{i=1}^{m}((h_\theta(x^{(i)})-y^i)x_j^{(i)})$ 

}

一些小技巧：

- 特征缩放，将特征全都变为较小且比例近与1，较小是为了使每步走的大一些，比例近乎于一，则$J(\theta)$的轮廓图的等高线更圆一些，我们知道越圆的椭圆切线越指向圆心，少走弯路。

  - 通常将特征值缩放到$(-1,+1)$之间，（并不是硬性规定
  - 当然如果你的特征值太小了，也要适度放大

- 均值归一化（mean normalization）

  - 用$x_i-u_i$代替$x_i$，使数据集中该特征的平均值为0
  - 当然绝对不能用在$x_0$上

  所以，你可以先这么处理你的特征：

  - $u_i$是你第$i$个特征的平均值
  - $s_i$是你第$i$个特征的范围大小
  - 把你的特征值变为：$x_i:= \frac{x_i-u_i}{s_i}$

- 如何选择学习率$\alpha$ 

  - 先跟着直觉选一个$\alpha$，在每一步迭代都输出计算出的$J(\theta)$，如果$J(\theta)$一直变小，没有上下横跳或一路变大就说明这个学习率可以，你也可以根据情况适度调大它；
  - 如果$J(\theta)$随着迭代次数的变大忽大忽小或一路变大不复返，说明学习率选大了，要调小
  - 还有确定何时停止迭代，两次迭代计算的$J(\theta)$的差值小于$\delta$，就看做已到局部最优点，停止迭代

  !["J-iterator"](D:\md笔记\机器学习\吴恩达\learning-rate.png)

  

如图就是$J(\theta)-\#iterator$的关系图，y轴到底会在哪里，以及曲线的缩放取决于你选择的初始$\theta$和学习率$\alpha$




> sigmoid?no,ReLU!
>
> 推荐视频：<https://www.bilibili.com/video/av15532370>

## 模型表示

标记说明：

$a_i^{(j)}$：第j层第i个“激励”，激励就是这个神经元输出的值

$\Theta^{(j)}$：第j层到第j+1层的权重矩阵，第i行就是前层到$a_i^(j+1)$的权重

![](.\pics\神经网络.png)

前向传播：输入层->隐藏层->输出层	$\vec{a^{(j+1)}}=sigmoid(\Theta^{(j)}\vec{a^{(j)}}+\vec{bias})$



> 神经网络对除输入层外的每个神经单元做的事就像是逻辑回归的$h_\theta(X)$所做的。
>
> 就最后一层来看，神经网络就像自己训练了逻辑回归的特征值一样。
>
> 当神经网络输出层有多个单元时，岂不及就像用一对多的方法进行逻辑回归多分类。

## 示例与直觉

> ==sigmoid函数g，g(4.6)=0.99，g(-4.6=0.01)==

与门的神经网络

![](.\pics\神经网络-and.png)

与门，或门拼成的异或非门（看起来就是多层逻辑模型，把层直观出来了，语法树...）

> 如果把神经网络看做是多层逻辑处理就可以理解它为什么可以处理xnor这种非线性关系了，逻辑可以把不几何相关的组（被逻辑二分出的）连接为一个。

![](.\pics\神经网络-xnor.png)


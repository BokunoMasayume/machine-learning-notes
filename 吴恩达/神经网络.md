> sigmoid?no,ReLU!
>
> 推荐视频：<https://www.bilibili.com/video/av15532370>

## 模型表示

标记说明：

$a_i^{(j)}$：第j层第i个“激励”，激励就是这个神经元输出的值

$\Theta^{(j)}$：第j层到第j+1层的权重矩阵，第i行就是前层到$a_i^(j+1)$的权重

![](.\pics\神经网络.png)

前向传播：输入层->隐藏层->输出层	$\vec{a^{(j+1)}}=sigmoid(\Theta^{(j)}\vec{a^{(j)}}+\vec{bias})$



> 神经网络对除输入层外的每个神经单元做的事就像是逻辑回归的$h_\theta(X)$所做的。
>
> 就最后一层来看，神经网络就像自己训练了逻辑回归的特征值一样。
>
> 当神经网络输出层有多个单元时，岂不及就像用一对多的方法进行逻辑回归多分类。

## 示例与直觉

> ==sigmoid函数g，g(4.6)=0.99，g(-4.6=0.01)==

与门的神经网络

![](.\pics\神经网络-and.png)

与门，或门拼成的异或非门（看起来就是多层逻辑模型，把层直观出来了，语法树...）

> 如果把神经网络看做是多层逻辑处理就可以理解它为什么可以处理xnor这种非线性关系了，逻辑可以把不几何相关的组（被逻辑二分出的）连接为一个。

![](.\pics\神经网络-xnor.png)

## 优化（学习）算法

### 代价函数

标记说明：

$L​$：神经网络的总层数

$s_l$：第l层上神经元的个数

$s_L$：输出层的神经元个数

$s_L=1$--------二元分类

$s_L=K$-------K元分类



> 就像我们之前说的相邻两层间的神经网络所作的就像逻辑回归一样，只不过逻辑回归输出的是标量的0\~1，神经网络输出的是每项都是0\~1的向量。而隐藏起隐藏项之后，神经网络和一对多的多分类逻辑回归是一样的，神经网络的$\Theta$就相当于每行是第i个逻辑回归参数向量的转置$\theta_i^T$

先看一下逻辑回归的代价函数：$J(\theta)=-\frac{1}{m}[\sum_{i=1}^m y^{(i)}log(h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2​$ （包含正则项）

神经网络和它的区别只在于神经网络的输出（或者说下一层的“激励”）是一个向量，推广一下，逻辑回归的向量形式应为：$J(\Theta)=-\frac{1}{m}[\sum_{i=1}^m \vec{y^T}\vec{log(h_\Theta(x^{(i)})}+\vec{(1-y)^T}\vec{log(1-h_\Theta(x^{(i)}))}]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2$

拆分一下就是：$J(\theta)=-\frac{1}{m}[\sum_{i=1}^m\sum_{k=1}^Ky^{(i)}_klog(h_\theta(x^{(i)})_k+(1-y^{(i)}_k)log(1-h_\theta(x^{(i)}))_k]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2$

其中：$h_\Theta(x)\in\R^K$（K个类别）     $(h_\Theta(x))_i=$第i个输出



### 反向传播算法

>[推荐视频](https://www.bilibili.com/video/av16577449)
>
>视频中的思想可能会有帮助

直观上说 $\delta_j^{l}$ 是第l层第j个节点的”误差“ 

比如对输出层（例子中是第4层），$\delta_j^{(4)}=a_j^{(4)}-y_j$，其中，$a_j^{(4)}=(h_\theta(x))_j$

向量形式为：$\delta^{(4)}=a^{(4)}-y$

$\delta^{(3)}=(\Theta^{(3)})^T\delta^{(4)} \ .*\ g'(z^{(3)})$，其中$.*$表示把两个向量同位置相乘得到新向量的运算

$\delta^{(2)}=(\Theta^{(2)})^T\delta^{(3)} \ .*\ g'(z^{(2)})$

ps: $g(z^{(i)})=\frac{1}{}1+e^{-z} \ \  \ g'(z^{(i)})=a^{(i)}.*(1-a^{(i)})$

> 直觉理解一下：按照推荐视频的观点，权重越大对整体影响越大，所以在上一层(l-1)中对$a_i^{(l)}$的权重越大的激励$a_j^{(l-1)}$应对$\delta_i^{(l)}$负更多比例的责任，或者说影响更大，即比例为$\Theta_{ij}$，责任为$\Theta_{ij}\delta_i^{(l)}$，则$a_j^{(l-1)}$的总责任是$\Theta_{i\_}\delta^{(l)}$。
>
> 以上是权重的责任分配（影响分配）。上式$g'(z^{(i)})$表示i-1层要改变它的难易程度，越容易的责任越大 ，或者说，对结果的影响越大。
>
> 故而有上式。

神奇的是：$\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)=a_j^{(l)}\delta_i^{(l+1)}​$（忽略正则化项，maybe单样本）

> 越”亮“的神经元对后一层影响越大，故乘$a_j^{(l)}​$
>
> 还有梯度，根据推荐视频，我们知道偏导越大的项对函数总体的影响越大（必经改变单位的值，使函数值上升最大嘛），梯度方向更照顾有更大偏导的方向来获得最快的函数增长速度，但关键是`最快的函数增长速度`，所以偏导=责任（或者说影响力），似乎还挺符合直觉的。
>
> ps:某一神经元的影响力由什么组成：实力/出力/帮助力（$a_i^{(l)}$ ），潜力（$g'(a_i^{(l)})$），人脉（$\Theta_{\_i}$）
>
> 帮助力大的前辈和潜力大的后辈，如果关系（人脉）还很好，将会对结果有大的正向改变，
>
> 但可惜这是个黑帮组织，潜力和帮助指的是杀人放火的技能（代价函数），并且正要努力洗白（最优化），所以要反向操作（$-\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta)\ ,\ \ -a_j^{(l)}\delta_i^{(l+1)}$）。

#### 反向传播使用方法

![](.\pics\反向传播过程.png)

之后使用优化算法（比如梯度下降法）优化参数。（上图加入正则项时似乎没有乘上$\frac{1}{m}$）

> 在这篇笔记中，我有时将偏置单独列为$bias​$，有时又融合在$\Theta​$和$a​$中，因为我在做笔记的时候参考了两个系列的视频（另一个我已经列出来了），希望当我下次看到的时候不要感到迷茫。

#### 反向传播微积分理解

$\delta_j^{(l)}​$：$a_j^{(l)}​$的代价的“误差”

就是说，$\delta_j^{(l)}=\frac{\partial}{\partial z^{(l)}}cost(i)​$，（i是第i个样本）

其中$cost(i)= y^{(i)}log(h_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))=y^{(i)}log(g(z^{(i)})+(1-y^{(i)})log(1-g(z^{(i)}))$

......下图$\delta$为什么没乘导数?

![](.\pics\反向-1.png)